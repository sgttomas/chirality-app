# UNSAID

If you collect everything that was left **UNSAID**, you don’t get a footnote—you get the **risk register and incentive map** that actually determines whether “AI in professional engineering” is safe.

## What “UNSAID” adds up to

- **Polish ≠ proof**: polished output is not evidence of correct engineering; it can look “done” before intent, assumptions, interfaces, and verification are settled.
- **Dominant failure mode**: over-trust and rubber‑stamp review—“human‑in‑the‑loop” becomes “human skims the plausible thing.”
- **Training crisis (“missing rungs”)**: if agents eat the junior work, you can hollow out the profession’s pipeline of competence and independent checking.
- **Documentation paradox**: “perfect” audit trails that are effectively unreadable—write‑only provenance that’s technically auditable but practically incomprehensible.
- **Incentives problem**: market pressure rewards speed and underbidding, nudging teams toward unsafe shortcuts until a failure forces external regulation.
- **Liability truth**: “the AI suggested it” won’t be a defense; expect stronger requirements for reproducible records, evidence packs, and disciplined gates.
- **Trust/public perception**: adoption isn’t only technical; credibility, governance, and social license become part of the engineering system.
- **Human factors are safety‑critical**: training, resistance, adaptation, and support systems are not “change management” extras—they’re part of risk control.

## The one-line synthesis

**Engineering isn’t content production. AI doesn’t change duty of care—only the failure modes.**
