import OpenAI from 'openai';
import { Modality } from '../../domain/station';

export interface LLMServiceConfig {
  apiKey: string;
  model?: string;
  maxTokens?: number;
  temperature?: number;
  disabled?: boolean; // If true, generateSemanticContent returns deterministic stubs
}

export interface SemanticContext {
  problemStatement?: string;
  dataSheet?: string;
  standardProcedure?: string;
  guidanceDocument?: string;
  evaluationChecklist?: string;
  solutionStatements?: string;
  matrixElements?: string[];
  iterationNumber?: number;
}

export interface LLMRequest {
  modality: Modality;
  operation: string;
  context: SemanticContext;
  matrixGuidance?: string[];
  systemPrompt?: string;
}

export interface LLMResponse {
  content: string;
  tokens: {
    prompt: number;
    completion: number;
    total: number;
  };
  model: string;
  finishReason: string;
}

export class LLMService {
  private openai: OpenAI;
  private config: Required<LLMServiceConfig>;

  constructor(config: LLMServiceConfig) {
    const model = config.model || process.env.OPENAI_MODEL;
    
    // Only require model if not disabled
    if (!config.disabled && !model) {
      throw new Error('OPENAI_MODEL environment variable is required. Set it in your .env.local file.');
    }

    this.config = {
      model: model || 'disabled-model',
      maxTokens: 2000,
      temperature: 0.7,
      disabled: false,
      ...config
    } as Required<LLMServiceConfig>;
    
    this.openai = new OpenAI({
      apiKey: this.config.apiKey
    });
  }

  async generateSemanticContent(request: LLMRequest): Promise<LLMResponse> {
    // If disabled, return deterministic stub without network call
    if (this.config.disabled) {
      const stubContent = this.generateStubContent(request.modality, request.operation);
      return {
        content: stubContent,
        tokens: {
          prompt: 0,
          completion: 0,
          total: 0
        },
        model: this.config.model || 'disabled-stub',
        finishReason: 'disabled'
      };
    }

    const systemPrompt = this.buildSystemPrompt(request.modality, request.operation);
    const userPrompt = this.buildUserPrompt(request);

    try {
      const response = await this.openai.chat.completions.create({
        model: this.config.model,
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: userPrompt }
        ],
        max_tokens: this.config.maxTokens,
        temperature: this.config.temperature
      });

      const choice = response.choices[0];
      if (!choice?.message?.content) {
        throw new Error('No content generated by LLM');
      }

      return {
        content: choice.message.content,
        tokens: {
          prompt: response.usage?.prompt_tokens || 0,
          completion: response.usage?.completion_tokens || 0,
          total: response.usage?.total_tokens || 0
        },
        model: response.model,
        finishReason: choice.finish_reason || 'unknown'
      };

    } catch (error) {
      throw new Error(`LLM generation failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  private generateStubContent(modality: Modality, operation: string): string {
    const stubs: Record<Modality, Record<string, string>> = {
      problem: {
        'Problem Statement Analysis (J Operation)': 'STUB: Problem analysis for testing purposes. This content represents structured problem categorization without LLM processing.',
        default: 'STUB: Problem modality content for testing purposes.'
      },
      systematic: {
        'Data Sheet Generation (DS from Matrix C)': 'STUB: Data sheet content for testing purposes. This represents systematic data requirements without LLM processing.',
        default: 'STUB: Systematic modality content for testing purposes.'
      },
      process: {
        'Standard Procedure Generation (SP from Matrix D)': 'STUB: Standard procedure content for testing purposes. This represents process workflow without LLM processing.',
        'Evaluation Checklist Generation (EC from Matrix E)': 'STUB: Evaluation checklist content for testing purposes. This represents process validation without LLM processing.',
        default: 'STUB: Process modality content for testing purposes.'
      },
      epistemic: {
        'Guidance Document Generation (GD from Matrix X)': 'STUB: Guidance document content for testing purposes. This represents epistemic guidance without LLM processing.',
        default: 'STUB: Epistemic modality content for testing purposes.'
      },
      alethic: {
        default: 'STUB: Alethic modality content for testing purposes.'
      },
      resolution: {
        'Final Resolution': 'STUB: Final resolution content for testing purposes. This represents resolution synthesis without LLM processing.',
        default: 'STUB: Resolution modality content for testing purposes.'
      }
    };

    return stubs[modality][operation] || stubs[modality].default || `STUB: ${modality} - ${operation}`;
  }

  private buildSystemPrompt(modality: Modality, operation: string): string {
    const modalityPrompts: Record<Modality, string> = {
      problem: `You are analyzing and categorizing a problem statement. Your role is to parse the problem and organize it systematically.`,
      systematic: `You are generating systematic documentation. Your role is to structure and organize information in a methodical, comprehensive way.`,
      process: `You are designing processes and procedures. Your role is to create actionable workflows and operational guidance.`,
      epistemic: `You are performing knowledge analysis and validation. Your role is to examine, verify, and integrate knowledge claims.`,
      alethic: `You are making necessity and possibility judgments. Your role is to determine what must be true and what could be possible.`,
      resolution: `You are synthesizing final resolutions. Your role is to integrate all prior work into a coherent, complete solution.`
    };

    return `${modalityPrompts[modality]}

Operation: ${operation}

Follow the principles of semantic valley traversal - each step should maintain ontological consistency while building toward resolution. Generate content that is precise, actionable, and semantically coherent with the ${modality} modality.`;
  }

  private buildUserPrompt(request: LLMRequest): string {
    let prompt = '';

    // Add context sections
    if (request.context.problemStatement) {
      prompt += `## Problem Statement\n${request.context.problemStatement}\n\n`;
    }

    if (request.context.dataSheet) {
      prompt += `## Data Sheet\n${request.context.dataSheet}\n\n`;
    }

    if (request.context.standardProcedure) {
      prompt += `## Standard Procedure\n${request.context.standardProcedure}\n\n`;
    }

    if (request.context.guidanceDocument) {
      prompt += `## Guidance Document\n${request.context.guidanceDocument}\n\n`;
    }

    if (request.context.evaluationChecklist) {
      prompt += `## Evaluation Checklist\n${request.context.evaluationChecklist}\n\n`;
    }

    if (request.context.solutionStatements) {
      prompt += `## Solution Statements\n${request.context.solutionStatements}\n\n`;
    }

    // Add matrix guidance if provided
    if (request.matrixGuidance && request.matrixGuidance.length > 0) {
      prompt += `## Matrix Guidance\n${request.matrixGuidance.join('\n')}\n\n`;
    }

    // Add iteration context
    if (request.context.iterationNumber) {
      prompt += `## Iteration Context\nThis is iteration ${request.context.iterationNumber} - refine and improve upon the existing documents.\n\n`;
    }

    prompt += `## Task\nGenerate the ${request.operation} following the ${request.modality} modality principles.`;

    return prompt;
  }

  async validateApiKey(): Promise<boolean> {
    try {
      await this.openai.models.list();
      return true;
    } catch {
      return false;
    }
  }
}